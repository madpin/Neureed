# Redis Cache Analysis and Recommendations

**Date:** November 21, 2025  
**Status:** âš ï¸ Significant improvements needed

## Executive Summary

The NeuReed application has a solid Redis cache infrastructure in place but is **underutilizing** it. Only ~20% of the defined cache keys are actively used, and several high-cost operations (semantic search, related articles, feed queries) are not cached at all. This results in:

- Unnecessary database load
- Slower response times for repeat queries
- Higher compute costs for vector operations
- Wasted LLM API costs for regenerating summaries

**Potential Impact of Improvements:**
- 60-80% reduction in database queries
- 50-70% faster response times for cached queries
- Significant reduction in vector search computation
- Better user experience with instant results

---

## Current Implementation

### âœ… What's Working Well

1. **Solid Foundation**
   - Clean abstraction layer (`cache-service.ts`)
   - Singleton Redis client with proper connection handling
   - Structured cache key namespacing
   - TTL management by content type
   - Pipeline support for batch operations
   - Statistics tracking (hits, misses, errors)

2. **Well-Implemented Caches**
   - **Article Scores** (1 hour TTL)
     - Cached per user-article combination
     - Batch fetching with `cacheGetMany`
     - Proper invalidation on pattern changes
   - **LLM Summaries** (7 days TTL)
     - Article summaries
     - Key points
     - Topic detection
   - **Cache Invalidation**
     - Pattern-based invalidation for users
     - Article-specific invalidation

3. **Good Patterns**
   - Cache-aside pattern with `cacheGetOrSet`
   - Graceful degradation (returns null on Redis failure)
   - Environment-based enable/disable

---

## âš ï¸ Critical Issues

### 1. **Semantic Search NOT Cached**
**Impact:** HIGH | **Effort:** LOW

**Problem:**
```typescript
// src/lib/services/semantic-search-service.ts
export async function searchSimilarArticles(query, options) {
  const { embedding } = await generateEmbedding(query); // Always generates
  return await searchByEmbedding(embedding, options);  // Always queries DB
}
```

**Why This Hurts:**
- Semantic search is one of the most expensive operations
- Generates embeddings for every search (API cost or compute time)
- Vector similarity searches are computationally expensive
- Same queries repeated often (e.g., "AI news", "javascript tutorials")

**Cache Keys Already Defined:**
```typescript
semanticSearch: (query: string, limit: number) =>
  `search:${Buffer.from(query).toString("base64").substring(0, 50)}:${limit}`,
```

**Cost:** ~1000 tokens per search query + vector operation (~50-100ms)

---

### 2. **Related Articles NOT Cached**
**Impact:** HIGH | **Effort:** LOW

**Problem:**
```typescript
// src/lib/services/semantic-search-service.ts
export async function findRelatedArticles(articleId, options) {
  // Always performs vector similarity search
  const results = await prisma.$queryRawUnsafe<Array<...>>(query, ...params);
}
```

**Why This Hurts:**
- Related articles rarely change (static for historical articles)
- Vector operations on every request
- Same article IDs requested repeatedly

**Cache Keys Already Defined:**
```typescript
relatedArticles: (articleId: string, limit: number) =>
  `related:${articleId}:${limit}`,
```

**Should Be:** 24-hour TTL (longer for articles >1 week old)

---

### 3. **User Patterns Fetched From DB Every Time**
**Impact:** MEDIUM | **Effort:** LOW

**Problem:**
```typescript
// src/lib/services/pattern-detection-service.ts
export async function getUserPatternsMap(userId: string) {
  const patterns = await getUserPatterns(userId); // Always DB query
  return new Map(patterns.map((p) => [p.keyword, p.weight]));
}
```

**Why This Hurts:**
- User patterns queried on EVERY article scoring operation
- Article scoring can happen 20-50 times per page load
- Patterns only updated on user feedback (infrequent)

**Cache Keys Defined But Unused:**
```typescript
userPatterns: (userId: string) => `patterns:${userId}`,
userPatternsMap: (userId: string) => `patterns:map:${userId}`,
```

**Recommended TTL:** 15 minutes (already defined in `CacheTTL`)

---

### 4. **Feed Metadata NOT Cached**
**Impact:** MEDIUM | **Effort:** LOW

**Problem:**
```typescript
// Feed queries hit database every time
// No caching in feed-service.ts
```

**Why This Hurts:**
- Feed metadata rarely changes
- Feed list queried on every navigation
- Article lists include feed metadata (N+1 pattern)

**Cache Keys Defined But Unused:**
```typescript
feedMetadata: (feedId: string) => `feed:${feedId}`,
feedArticles: (feedId: string, page: number) => `feed:articles:${feedId}:${page}`,
```

**Recommended TTL:** 1 hour for metadata, 5 minutes for articles

---

### 5. **User Subscriptions NOT Cached**
**Impact:** MEDIUM | **Effort:** LOW

**Problem:**
```typescript
// app/api/articles/route.ts
let subscribedFeedIds = await getUserFeedIds(user.id); // Always DB query
```

**Why This Hurts:**
- User subscriptions queried on EVERY article list request
- Subscriptions change infrequently
- Multiple queries per user session

**Missing Cache Key:**
```typescript
userSubscriptions: (userId: string) => `subs:${userId}`,
```

**Recommended TTL:** 15 minutes

---

### 6. **Categories NOT Cached**
**Impact:** LOW-MEDIUM | **Effort:** LOW

**Problem:**
- Category lists fetched from DB every time
- Category membership rarely changes
- Grouped feed views query categories + feeds + articles

**Missing Cache Keys:**
```typescript
userCategories: (userId: string) => `categories:${userId}`,
categoryFeeds: (userId: string, categoryId: string) => 
  `category:${userId}:${categoryId}:feeds`,
```

**Recommended TTL:** 30 minutes

---

### 7. **Embedding Cost Tracking In-Memory**
**Impact:** LOW | **Effort:** LOW

**Problem:**
```typescript
// src/lib/services/embedding-cost-tracker.ts
let costHistory: CostEntry[] = []; // Lost on server restart
let totalTokens = 0;
let totalCost = 0;
```

**Why This Hurts:**
- Cost data lost on deployment/restart
- Can't track costs across multiple instances
- No historical analysis

**Should Use:**
- Redis for short-term accumulation
- Database for long-term storage
- Periodic flush to DB

---

### 8. **Inefficient Pattern Deletion**
**Impact:** LOW | **Effort:** MEDIUM

**Problem:**
```typescript
// src/lib/cache/cache-service.ts
export async function cacheDeletePattern(pattern: string) {
  const keys = await client.keys(pattern); // BLOCKS Redis!
  if (keys.length === 0) return 0;
  const deleted = await client.del(...keys);
}
```

**Why This Hurts:**
- `KEYS` command blocks Redis for all clients
- Dangerous in production with many keys
- Can cause timeouts

**Should Use:**
```typescript
// Use SCAN for non-blocking iteration
let cursor = 0;
do {
  const reply = await client.scan(cursor, 'MATCH', pattern, 'COUNT', 100);
  cursor = parseInt(reply[0]);
  if (reply[1].length > 0) {
    await client.del(...reply[1]);
  }
} while (cursor !== 0);
```

---

## ðŸŽ¯ Recommended Implementation Priorities

### Priority 1: HIGH IMPACT, LOW EFFORT (Immediate)

#### 1.1 Cache Semantic Search Results
```typescript
// src/lib/services/semantic-search-service.ts
export async function searchSimilarArticles(
  query: string,
  options: SemanticSearchOptions = {},
  provider?: EmbeddingProvider
): Promise<SearchResult[]> {
  const { limit = 10, minScore = 0.7, feedIds, since, until } = options;
  
  // Generate cache key based on all parameters
  const cacheKey = CacheKeys.semanticSearch(
    `${query}:${limit}:${minScore}:${feedIds?.join(',') || 'all'}:${since || ''}:${until || ''}`
  );
  
  return await cacheGetOrSet(
    cacheKey,
    async () => {
      const { embedding } = await generateEmbedding(query, provider);
      return await searchByEmbedding(embedding, { limit, minScore, feedIds, since, until });
    },
    CacheTTL.semanticSearch // 30 minutes
  );
}
```

**Expected Impact:** 70-80% cache hit rate after warmup

---

#### 1.2 Cache Related Articles
```typescript
// src/lib/services/semantic-search-service.ts
export async function findRelatedArticles(
  articleId: string,
  options: Omit<SemanticSearchOptions, "feedIds"> = {}
): Promise<SearchResult[]> {
  const { limit = 10, minScore = 0.7, excludeSameFeed = false } = options;
  
  const cacheKey = CacheKeys.relatedArticles(
    `${articleId}:${limit}:${minScore}:${excludeSameFeed}`
  );
  
  return await cacheGetOrSet(
    cacheKey,
    async () => {
      // Existing implementation
      return /* vector search results */;
    },
    CacheTTL.relatedArticles // 24 hours
  );
}
```

**Expected Impact:** 90%+ cache hit rate (articles rarely change)

---

#### 1.3 Cache User Patterns
```typescript
// src/lib/services/pattern-detection-service.ts
export async function getUserPatternsMap(userId: string): Promise<Map<string, number>> {
  const cacheKey = CacheKeys.userPatternsMap(userId);
  
  return await cacheGetOrSet(
    cacheKey,
    async () => {
      const patterns = await getUserPatterns(userId);
      return new Map(patterns.map((p) => [p.keyword, p.weight]));
    },
    CacheTTL.userPatterns // 15 minutes
  );
}
```

**Expected Impact:** Reduces DB queries by 95% for pattern lookups

---

#### 1.4 Cache User Subscriptions
```typescript
// Create new service: src/lib/services/user-subscription-cache-service.ts
import { CacheKeys, CacheTTL } from "@/lib/cache/cache-keys";
import { cacheGetOrSet, cacheDelete } from "@/lib/cache/cache-service";
import { prisma } from "@/lib/db";

export async function getUserFeedIdsWithCache(userId: string): Promise<string[]> {
  const cacheKey = CacheKeys.userSubscriptions(userId);
  
  return await cacheGetOrSet(
    cacheKey,
    async () => {
      const subscriptions = await prisma.user_feeds.findMany({
        where: { userId, isSubscribed: true },
        select: { feedId: true },
      });
      return subscriptions.map(s => s.feedId);
    },
    900 // 15 minutes
  );
}

export async function invalidateUserSubscriptions(userId: string): Promise<void> {
  await cacheDelete(CacheKeys.userSubscriptions(userId));
}

// Update cache keys
export const CacheKeys = {
  // ... existing keys
  userSubscriptions: (userId: string) => `subs:${userId}`,
};
```

**Usage:**
```typescript
// app/api/articles/route.ts
import { getUserFeedIdsWithCache } from "@/lib/services/user-subscription-cache-service";

let subscribedFeedIds = await getUserFeedIdsWithCache(user.id);
```

**Expected Impact:** 90%+ cache hit rate, faster article listing

---

### Priority 2: MEDIUM IMPACT, LOW EFFORT (Week 1)

#### 2.1 Cache Feed Metadata
```typescript
// src/lib/services/feed-service.ts
export async function getFeedWithCache(feedId: string) {
  const cacheKey = CacheKeys.feedMetadata(feedId);
  
  return await cacheGetOrSet(
    cacheKey,
    async () => await getFeed(feedId),
    CacheTTL.feedMetadata // 1 hour
  );
}
```

---

#### 2.2 Cache User Categories
```typescript
// src/lib/services/category-service.ts
export async function getUserCategoriesWithCache(userId: string) {
  const cacheKey = CacheKeys.userCategories(userId);
  
  return await cacheGetOrSet(
    cacheKey,
    async () => await getUserCategories(userId),
    1800 // 30 minutes
  );
}

// Update cache keys
export const CacheKeys = {
  // ... existing keys
  userCategories: (userId: string) => `categories:${userId}`,
  categoryFeeds: (userId: string, categoryId: string) => 
    `category:${userId}:${categoryId}:feeds`,
};
```

---

#### 2.3 Fix Inefficient Pattern Deletion
```typescript
// src/lib/cache/cache-service.ts
export async function cacheDeletePattern(pattern: string): Promise<number> {
  const client = getRedisClient();
  if (!client) return 0;

  try {
    let cursor = 0;
    let totalDeleted = 0;
    
    do {
      const reply = await client.scan(
        cursor, 
        'MATCH', pattern, 
        'COUNT', 100
      );
      
      cursor = parseInt(reply[0]);
      const keys = reply[1];
      
      if (keys.length > 0) {
        const deleted = await client.del(...keys);
        totalDeleted += deleted;
        stats.deletes += deleted;
      }
    } while (cursor !== 0);
    
    return totalDeleted;
  } catch (error) {
    stats.errors++;
    logger.error("Cache delete pattern error", { pattern, error });
    return 0;
  }
}
```

---

### Priority 3: TECHNICAL DEBT (Week 2)

#### 3.1 Move Embedding Costs to Redis/DB
```typescript
// src/lib/services/embedding-cost-tracker.ts
import { cacheIncrement, cacheGet, cacheSet } from "@/lib/cache/cache-service";
import { prisma } from "@/lib/db";

export async function trackEmbeddingCost(
  provider: string,
  tokens: number,
  operation: string = "embedding"
): Promise<void> {
  const cost = calculateCost(provider, tokens);
  const today = new Date().toISOString().split('T')[0];
  
  // Accumulate in Redis
  await Promise.all([
    cacheIncrement(`cost:tokens:${today}`, tokens),
    cacheIncrement(`cost:amount:${today}`, Math.round(cost * 1000000)), // Store as micro-dollars
    cacheIncrement(`cost:count:${today}`, 1),
  ]);
  
  // Flush to database periodically (via cron job)
}

export async function flushCostsToDatabase(): Promise<void> {
  // Move Redis accumulated costs to database for long-term storage
  // Run daily via cron job
}
```

---

#### 3.2 Add Cache Warming
```typescript
// src/lib/jobs/cache-warming-job.ts
import { createJobExecutor } from "./job-executor";
import { JobLogger } from "./job-logger";
import { cacheWarm } from "@/lib/cache/cache-service";
import { prisma } from "@/lib/db";

export const warmCacheJob = createJobExecutor(
  "cache-warming",
  async (logger: JobLogger) => {
    logger.info("Starting cache warming");
    
    // Warm popular semantic searches
    const popularSearches = ["AI", "javascript", "python", "react", "typescript"];
    for (const query of popularSearches) {
      try {
        await searchSimilarArticles(query, { limit: 10 });
        logger.info(`Warmed search cache for: ${query}`);
      } catch (error) {
        logger.error(`Failed to warm search for ${query}`, { error });
      }
    }
    
    // Warm recent articles
    const recentArticles = await prisma.articles.findMany({
      take: 50,
      orderBy: { publishedAt: 'desc' },
      select: { id: true },
    });
    
    for (const article of recentArticles) {
      try {
        await findRelatedArticles(article.id, { limit: 5 });
      } catch (error) {
        logger.error(`Failed to warm related articles for ${article.id}`, { error });
      }
    }
    
    logger.info("Cache warming completed");
  }
);
```

Run every 6 hours via cron.

---

#### 3.3 Add Request-Level Caching (DataLoader Pattern)
```typescript
// src/lib/cache/request-cache.ts
import { AsyncLocalStorage } from 'async_hooks';

const asyncLocalStorage = new AsyncLocalStorage<Map<string, any>>();

export function withRequestCache<T>(fn: () => Promise<T>): Promise<T> {
  const cache = new Map<string, any>();
  return asyncLocalStorage.run(cache, fn);
}

export async function requestCacheGet<T>(key: string): Promise<T | undefined> {
  const cache = asyncLocalStorage.getStore();
  return cache?.get(key);
}

export function requestCacheSet(key: string, value: any): void {
  const cache = asyncLocalStorage.getStore();
  cache?.set(key, value);
}

// Usage in API routes
export const GET = createHandler(
  async ({ query }) => {
    return withRequestCache(async () => {
      // Multiple calls to same function will use cached result
      const feed = await getFeedWithRequestCache(feedId);
    });
  }
);
```

---

## ðŸ“Š Expected Performance Improvements

### Database Load Reduction
| Operation | Before | After | Improvement |
|-----------|--------|-------|-------------|
| Semantic Search | 100% DB hits | 20-30% DB hits | **70-80% reduction** |
| Related Articles | 100% DB hits | 5-10% DB hits | **90-95% reduction** |
| User Patterns | 100% DB hits | 5% DB hits | **95% reduction** |
| User Subscriptions | 100% DB hits | 10% DB hits | **90% reduction** |
| Feed Metadata | 100% DB hits | 10-20% DB hits | **80-90% reduction** |

### Response Time Improvements
| Operation | Before | After (Cached) | Improvement |
|-----------|--------|----------------|-------------|
| Semantic Search | 200-500ms | 5-10ms | **95-98% faster** |
| Related Articles | 100-200ms | 2-5ms | **95-98% faster** |
| Article Listing | 150-300ms | 50-100ms | **50-70% faster** |
| Pattern Matching | 50-100ms | 2-5ms | **95-98% faster** |

### Cost Savings
- **Embedding API Calls:** 70-80% reduction in duplicate queries
- **Vector Operations:** 90% reduction in pgvector computations
- **Database Queries:** 60-80% overall reduction

---

## ðŸ” Monitoring & Observability

### Add Cache Metrics Dashboard
```typescript
// app/api/admin/cache/metrics/route.ts
export const GET = createHandler(async () => {
  const stats = getCacheStats();
  const info = await getCacheInfo();
  
  // Get cache breakdown by prefix
  const client = getRedisClient();
  const keyStats = {};
  
  for (const prefix of ['score:', 'search:', 'related:', 'patterns:', 'subs:']) {
    const keys = await client.keys(`${prefix}*`);
    keyStats[prefix] = keys.length;
  }
  
  return {
    overall: stats,
    redisInfo: info,
    keysByPrefix: keyStats,
    recommendations: generateRecommendations(stats, keyStats),
  };
}, { requireAdmin: true });
```

### Add Cache Hit Rate Alerts
- Alert if hit rate < 40% (should be 60-80%)
- Alert if error rate > 5%
- Alert if memory usage > 80%

---

## ðŸŽ¬ Implementation Roadmap

### Week 1: Quick Wins
- [ ] Day 1: Implement semantic search caching
- [ ] Day 2: Implement related articles caching
- [ ] Day 3: Implement user patterns caching
- [ ] Day 4: Implement user subscriptions caching
- [ ] Day 5: Testing & monitoring

### Week 2: Medium Priority
- [ ] Day 1-2: Implement feed metadata caching
- [ ] Day 3: Implement category caching
- [ ] Day 4: Fix pattern deletion (SCAN)
- [ ] Day 5: Testing & documentation

### Week 3: Technical Debt
- [ ] Day 1-2: Move embedding costs to Redis/DB
- [ ] Day 3: Implement cache warming
- [ ] Day 4-5: Request-level caching (optional)

---

## ðŸ“‹ Cache Configuration Best Practices

### Recommended TTL Strategy
```typescript
export const CacheTTL = {
  // Ultra-short (2 minutes) - rapidly changing data
  feedRefreshStatus: 2 * 60,
  
  // Short (5-15 minutes) - frequently updated
  userSubscriptions: 15 * 60,
  userPatterns: 15 * 60,
  feedArticles: 5 * 60,
  userCategories: 30 * 60,
  
  // Medium (30-60 minutes) - stable data
  articleScore: 60 * 60,
  feedMetadata: 60 * 60,
  userPreferences: 60 * 60,
  semanticSearch: 30 * 60,
  
  // Long (24 hours) - rarely changing
  articleEmbedding: 24 * 60 * 60,
  relatedArticles: 24 * 60 * 60,
  
  // Very long (7 days) - static content
  articleSummary: 7 * 24 * 60 * 60,
  articleKeyPoints: 7 * 24 * 60 * 60,
  articleTopics: 7 * 24 * 60 * 60,
};
```

### Cache Invalidation Events
```typescript
// When to invalidate cache
const invalidationTriggers = {
  userPatterns: ['user_feedback', 'pattern_update'],
  userSubscriptions: ['subscribe', 'unsubscribe'],
  userCategories: ['category_create', 'category_update', 'category_delete'],
  feedMetadata: ['feed_update', 'feed_settings_change'],
  articleScore: ['user_patterns_change'],
  relatedArticles: ['article_update', 'embedding_regenerate'],
};
```

---

## ðŸš¨ Production Considerations

### 1. Redis Memory Management
- **Set maxmemory:** 512MB-2GB depending on user base
- **Eviction policy:** `allkeys-lru` (Least Recently Used)
- **Monitor memory usage:** Alert at 80%

### 2. Connection Pooling
```typescript
// redis-client.ts
redisClient = new Redis(redisUrl, {
  maxRetriesPerRequest: 3,
  retryStrategy(times) {
    return Math.min(times * 50, 2000);
  },
  lazyConnect: true,
  // Add connection pooling
  maxRetriesPerRequest: 3,
  enableReadyCheck: true,
  keepAlive: 30000,
});
```

### 3. Cache Stampede Prevention
```typescript
// Prevent cache stampede for expensive operations
export async function cacheGetOrSetWithLock<T>(
  key: string,
  fetchFn: () => Promise<T>,
  ttl?: number
): Promise<T | null> {
  const cached = await cacheGet<T>(key);
  if (cached !== null) return cached;
  
  // Try to acquire lock
  const lockKey = `lock:${key}`;
  const locked = await client.set(lockKey, '1', 'EX', 10, 'NX');
  
  if (!locked) {
    // Another process is fetching, wait and retry
    await new Promise(resolve => setTimeout(resolve, 100));
    return await cacheGet<T>(key) || fetchFn();
  }
  
  try {
    const value = await fetchFn();
    if (value !== null && value !== undefined) {
      await cacheSet(key, value, ttl);
    }
    return value;
  } finally {
    await client.del(lockKey);
  }
}
```

---

## ðŸ“ˆ Success Metrics

### Measure After Implementation
1. **Cache Hit Rate:** Target 60-80% overall
2. **Response Times:** 50-70% improvement on cached paths
3. **Database Load:** 60-80% reduction in query volume
4. **API Costs:** 70% reduction in embedding generation
5. **User Experience:** Faster page loads, instant search results

### Monitoring Dashboard
- Cache hit rate by prefix
- Memory usage trends
- Eviction rates
- Error rates
- Top cached keys
- Slowest cache operations

---

## ðŸŽ¯ Conclusion

The NeuReed application has excellent cache infrastructure but is leaving significant performance on the table. Implementing these recommendations will result in:

- **Immediate wins** (Week 1): 70-80% database load reduction
- **Medium-term gains** (Week 2): Additional 10-15% performance improvement
- **Long-term benefits** (Week 3): Better observability and maintainability

**Estimated Implementation Time:** 2-3 weeks  
**Expected ROI:** 5-10x in reduced costs and improved UX  
**Risk Level:** Low (cache failures degrade gracefully to DB)

---

## ðŸ“š References

- [Redis Best Practices](https://redis.io/docs/manual/patterns/)
- [Cache-Aside Pattern](https://docs.microsoft.com/en-us/azure/architecture/patterns/cache-aside)
- [Preventing Cache Stampede](https://en.wikipedia.org/wiki/Cache_stampede)
- [SCAN vs KEYS in Redis](https://redis.io/commands/scan/)

