# Embedding Error Fix Summary

## Problem Identified

You were seeing repeated errors in production logs:

```
[ERROR] OpenAI embedding generation failed {"error":{}}
[ERROR] Failed to load local embedding model
Error: libonnxruntime.so.1.14.0: cannot open shared object file: No such file or directory
```

## Root Cause Analysis

1. **Admin panel behavior:** The admin embeddings configuration page (`/api/admin/embeddings/config` and `/api/admin/embeddings/provider`) automatically tests **both** embedding providers (OpenAI and local) on every GET request to show their status.

2. **OpenAI errors:** When `OPENAI_API_KEY` is not configured, OpenAI provider fails and logs errors.

3. **Local embedding errors:** When local embeddings are tested, `@xenova/transformers` tries to load native ONNX Runtime libraries (`libonnxruntime.so.1.14.0`) which aren't installed in your Nixpacks environment.

4. **Error logging:** Both failures were logged at ERROR level, causing log spam every time someone visited the admin panel.

## Solutions Implemented

### 1. Improved Error Logging (Immediate Fix)

**Changed files:**
- `src/lib/embeddings/local-provider.ts`
- `src/lib/embeddings/openai-provider.ts`
- `app/api/admin/embeddings/config/route.ts`
- `app/api/admin/embeddings/provider/route.ts`

**What changed:**
- Dependency errors (missing ONNX Runtime) now log at **DEBUG** level instead of ERROR
- Configuration errors (missing API keys) now log at **DEBUG** level instead of ERROR
- Admin endpoints catch and handle test failures gracefully
- Only unexpected errors are logged at ERROR level

**Result:** No more error log spam! Expected failures are silent unless debug logging is enabled.

### 2. Added ONNX Runtime Support (Long-term Fix)

**Changed file:** `nixpacks.toml`

**What changed:**
```toml
# Before
nixPkgs = ["nodejs_20", "openssl"]

# After
nixPkgs = ["nodejs_20", "openssl", "onnxruntime"]
```

**Result:** Native ONNX Runtime libraries are now available in production, so local embeddings can work with native backend (faster than WASM).

### 3. Improved WASM Backend Configuration

**Changed file:** `src/lib/embeddings/local-provider.ts`

**What changed:**
- More explicit WASM backend configuration
- Forces WASM execution providers
- Sets cache directory for model downloads
- Better error messages with hints

**Result:** If native ONNX Runtime isn't available, WASM backend should work as fallback.

### 4. Added Environment Variables

**Changed file:** `nixpacks.toml`

**What changed:**
```toml
TRANSFORMERS_CACHE = "./.cache/transformers"
USE_ONNX_WASM = "1"
```

**Result:** Better control over transformers.js behavior.

## Deployment Options

### Option A: Use OpenAI (RECOMMENDED)

**Best for:** Production environments where you want fast, reliable embeddings

**Setup in Dokploy:**
```bash
EMBEDDING_PROVIDER=openai
OPENAI_API_KEY=sk-your-actual-api-key
```

**Pros:**
- ‚ö° Very fast (100x faster than local)
- üéØ Better quality embeddings
- üí∞ Very cheap (~$0.13 per 1M tokens)
- üîß No infrastructure complexity
- ‚úÖ No errors in logs

**After setting:** Just redeploy (no rebuild needed)

### Option B: Use Local Embeddings with Native ONNX

**Best for:** Privacy-sensitive environments or zero-cost operation

**Setup:**
1. Commit and push the changes (already done in this session)
2. In Dokploy, trigger a **rebuild** (not just redeploy)
3. Wait for build to complete

**Pros:**
- üîí Complete data privacy
- üí∞ Free (no API costs)
- ‚úÖ No errors in logs (after rebuild)

**Cons:**
- üêå Slower than OpenAI
- üêò Uses more memory (~500MB-1GB)
- ‚è±Ô∏è Longer build time

### Option C: Use Local Embeddings with WASM

**Best for:** Testing or when native ONNX isn't available

**Setup:**
1. Commit and push the changes
2. Redeploy (no rebuild needed)

**Pros:**
- üîí Complete data privacy
- üí∞ Free
- ‚úÖ Works everywhere
- ‚úÖ No errors in logs

**Cons:**
- üêå Slowest option
- üêò Uses more memory

## Files Changed

| File | Changes |
|------|---------|
| `nixpacks.toml` | Added `onnxruntime` package, added env vars |
| `src/lib/embeddings/local-provider.ts` | Improved WASM config, debug logging, better error handling |
| `src/lib/embeddings/openai-provider.ts` | Debug logging for config errors |
| `app/api/admin/embeddings/config/route.ts` | Graceful provider test error handling |
| `app/api/admin/embeddings/provider/route.ts` | Graceful provider test error handling |
| `docs/PRODUCTION_EMBEDDING_FIX.md` | Comprehensive documentation |
| `QUICK_FIX_EMBEDDINGS.md` | Quick reference guide |

## Expected Log Output After Fix

### Before (with errors):
```
[2025-11-19T14:33:57.631Z] [ERROR] OpenAI embedding generation failed {"error":{}}
[2025-11-19T14:33:57.631Z] [INFO] Initializing local embedding model
[2025-11-19T14:33:57.697Z] [ERROR] Failed to load local embedding model
Error: libonnxruntime.so.1.14.0: cannot open shared object file
[2025-11-19T14:34:57.735Z] [ERROR] OpenAI embedding generation failed
[2025-11-19T14:34:57.735Z] [INFO] Initializing local embedding model
[2025-11-19T14:34:57.735Z] [ERROR] Failed to load local embedding model
```

### After (clean logs):
```
[2025-11-19T14:33:57.344Z] [INFO] Redis client connected
[2025-11-19T14:33:57.350Z] [INFO] Redis client ready
[2025-11-19T14:33:57.400Z] [INFO] Using OpenAI embedding provider
```

Or if using local embeddings:
```
[2025-11-19T14:33:57.631Z] [INFO] Initializing local embedding model
[2025-11-19T14:33:58.200Z] [INFO] Local embedding model loaded successfully {"backend":"native"}
```

## Next Steps

1. **Choose your deployment option** (A, B, or C above)
2. **Set environment variables** in Dokploy if using Option A
3. **Commit and push** the changes:
   ```bash
   git add .
   git commit -m "Fix: Embedding provider error logging and ONNX Runtime support"
   git push
   ```
4. **Deploy:**
   - Option A: Just redeploy
   - Option B or C: Rebuild in Dokploy
5. **Verify:** Check logs to confirm no more error spam

## Testing

After deployment, test by:

1. **Visit the admin panel** ‚Üí Settings ‚Üí Embeddings
2. **Check the logs** - should see no ERROR messages
3. **Try generating embeddings** for an article
4. **Verify it works** - check article has embedding in database

## Rollback Plan

If something goes wrong:

1. **Revert the commit:**
   ```bash
   git revert HEAD
   git push
   ```

2. **Or just set OpenAI provider:**
   ```bash
   EMBEDDING_PROVIDER=openai
   OPENAI_API_KEY=sk-your-key
   ```

3. **Redeploy**

## Additional Resources

- Full documentation: `docs/PRODUCTION_EMBEDDING_FIX.md`
- Quick reference: `QUICK_FIX_EMBEDDINGS.md`
- OpenAI pricing: https://openai.com/api/pricing/
- Transformers.js docs: https://huggingface.co/docs/transformers.js

## Questions?

**Q: Will this break anything?**
A: No. All changes are backward compatible and improve error handling.

**Q: Do I need to rebuild?**
A: Only if you want to use local embeddings with native ONNX. For OpenAI, just redeploy.

**Q: What if I don't set any environment variables?**
A: The system will default to local provider, which will now fail gracefully (debug logs only).

**Q: How much does OpenAI cost?**
A: Very little. ~$0.13 per 1 million tokens. For 10,000 articles, that's about $0.65.

**Q: Can I switch providers later?**
A: Yes! Just change the environment variable and redeploy. Or use the admin panel.

